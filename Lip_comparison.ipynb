{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1beac7e4-0e9c-4704-adbc-db3b465ea6ec",
   "metadata": {},
   "source": [
    "# Live Capturing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c68362-2d28-4400-b7d5-66ac6e9637d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#live video capture for 5 seconds\n",
    "import cv2\n",
    "import dlib\n",
    "import time\n",
    "\n",
    "def capture_live_lip_movements(output_path, fps=29, duration=3):\n",
    "    \"\"\"\n",
    "    Captures a live video of lip movements for a specified duration and saves it.\n",
    "\n",
    "    Args:\n",
    "        output_path: The path and filename to save the output video.\n",
    "        fps: The desired frame rate for the output video (default: 29).\n",
    "        duration: The duration in seconds to capture the video (default: 3).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load face detector and facial landmarks predictor from dlib\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(r\"C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "    # Open video capture object (0 for default webcam)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Get video frame dimensions\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "\n",
    "    # Create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    start_time = time.time()  # Capture start time\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if video capture fails\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Check if capture duration has been reached\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time >= duration:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = detector(gray_frame)\n",
    "\n",
    "        # Process only if a face is detected\n",
    "        if len(faces) > 0:\n",
    "            face = faces[0]  # Assuming only one face is present\n",
    "\n",
    "            # Detect facial landmarks\n",
    "            shape = predictor(gray_frame, face)\n",
    "\n",
    "            # Extract lip landmarks (indices based on dlib's model)\n",
    "            lip_landmarks = [(shape.part(i).x, shape.part(i).y) for i in range(48, 58)]\n",
    "\n",
    "            # You can add additional processing or visualization of the lip landmarks here\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Live Lip Movements', frame)\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Destroy all OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "output_path = r\"C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\outputs\\Extractions\\Demo\\video_captured10.mp4\"\n",
    "capture_live_lip_movements(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283924cc-9e91-47d1-bd0a-cd216cc0462e",
   "metadata": {},
   "source": [
    "# Lip Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0140117-099e-4ee7-b9e6-0c43f1c05fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete. Output saved to: C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\outputs\\Extractions\\Demo\\test.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "# Load face detector and facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(r\"C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Define video file path (replace with your video path)\n",
    "video_path = r\"C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\outputs\\Extractions\\Demo\\video_captured10.mp4\"\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video file:\", video_path)\n",
    "    exit()\n",
    "\n",
    "# Get video properties (optional, adjust output FPS if needed)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# --- Optional: Define video output properties for lower face region ---\n",
    "# Adjust codec (fourcc) and filename as needed\n",
    "output_path = r\"C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\outputs\\Extractions\\Demo\\test.mp4\"\n",
    "output_fps = 29\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_path, fourcc, output_fps, (112, 80))\n",
    "\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        # Predict facial landmarks\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Extract lower face landmarks (below the nose until the jaw)\n",
    "        lower_face_landmarks = landmarks.parts()[48:58]\n",
    "\n",
    "        # Convert lower face landmarks to NumPy arrays\n",
    "        lower_face_points = np.array([(p.x, p.y) for p in lower_face_landmarks], dtype=np.int32)\n",
    "\n",
    "        # Get the bounding box for the lower part of the face\n",
    "        x, y, w, h = cv2.boundingRect(lower_face_points)\n",
    "\n",
    "        # Crop the lower part of the face\n",
    "        lower_face = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize the lower face to the desired frame size (112x80)\n",
    "        lower_face = cv2.resize(lower_face, (112, 80))\n",
    "\n",
    "        # Write the extracted lower face region to video (optional)\n",
    "        out.write(lower_face)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()  # Release the VideoWriter object (if used)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Video processing complete. Output saved to:\", output_path)  # Modify if not saving extracted lip region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec10727-fbf1-49fa-a00e-a1e675f8f458",
   "metadata": {},
   "source": [
    "# Lip Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e23285-5aa0-4efd-a51f-c586cf2d4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working Program \n",
    "#same above program with slow rate of video\n",
    "import cv2\n",
    "\n",
    "def compare_lip_movements(video_path1, video_path2, slow_factor=2):\n",
    "  \"\"\"\n",
    "  Compares two videos side-by-side and displays them in a slow-motion manner.\n",
    "\n",
    "  Args:\n",
    "      video_path1: Path to the first video file.\n",
    "      video_path2: Path to the second video file.\n",
    "      slow_factor: An integer (default: 2) specifying how much to slow down the playback. Higher values result in slower playback.\n",
    "  \"\"\"\n",
    "\n",
    "  # Open video files\n",
    "  cap1 = cv2.VideoCapture(video_path1)\n",
    "  cap2 = cv2.VideoCapture(video_path2)\n",
    "\n",
    "  while True:\n",
    "    # Read frames from the videos\n",
    "    ret1, frame1 = cap1.read()\n",
    "    ret2, frame2 = cap2.read()\n",
    "\n",
    "    # Break the loop if the videos end\n",
    "    if not ret1 or not ret2:\n",
    "      break\n",
    "\n",
    "    # Display the frames side by side\n",
    "    comparison_frame = cv2.hconcat([frame1, frame2])\n",
    "\n",
    "    # Show the frame for a longer duration to slow down playback\n",
    "    cv2.imshow('Lip Movements Comparison (Slow Motion)', comparison_frame)\n",
    "    cv2.waitKey(int(1000 / slow_factor))  # Wait for specified milliseconds based on slow_factor\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "  # Release the video capture objects\n",
    "  cap1.release()\n",
    "  cap2.release()\n",
    "\n",
    "  # Destroy all OpenCV windows\n",
    "  cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage with slow motion effect\n",
    "video_path1 = r\"C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\outputs\\Extractions\\Demo\\hello_2.mp4\"\n",
    "video_path2 = r\"C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\outputs\\Extractions\\Demo\\hello_4.mp4\"\n",
    "compare_lip_movements(video_path1, video_path2, slow_factor=4)  # Adjust slow_factor as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb43a5-a0d8-4302-903f-08d952df163e",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beccfbff-0135-4663-85ba-7f5b75ced853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def calculate_ssim(video_path1, video_path2):\n",
    "    # Open video files\n",
    "    cap1 = cv2.VideoCapture(video_path1)\n",
    "    cap2 = cv2.VideoCapture(video_path2)\n",
    "\n",
    "    # Initialize SSIM values\n",
    "    ssim_values = []\n",
    "\n",
    "    while True:\n",
    "        # Read frames from the videos\n",
    "        ret1, frame1 = cap1.read()\n",
    "        ret2, frame2 = cap2.read()\n",
    "\n",
    "        # Break the loop if the videos end\n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "\n",
    "        # Convert frames to grayscale\n",
    "        gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate SSIM between frames\n",
    "        score, _ = ssim(gray_frame1, gray_frame2, full=True)\n",
    "        ssim_values.append(score)\n",
    "\n",
    "    # Release the video capture objects\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "    \n",
    "    \n",
    "\n",
    "    # Calculate average SSIM value\n",
    "    average_ssim = np.mean(ssim_values)\n",
    "    \n",
    "    # Provide feedback based on the average SSIM\n",
    "    if average_ssim > 0.8:\n",
    "        print(\"The lip movements are very similar.\")\n",
    "    elif average_ssim > 0.6:\n",
    "        print(\"The lip movements are moderately similar.\")\n",
    "    else:\n",
    "        print(\"The lip movements are significantly different.\")\n",
    "    \n",
    "# Example usage\n",
    "video_path1 = r\"C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\outputs\\Extractions\\Demo\\hello_2.mp4\"\n",
    "video_path2 = r\"C:\\Users\\Dhanush N\\Desktop\\Mini project\\Dataset\\outputs\\Extractions\\Demo\\hello_4.mp4\"\n",
    "calculate_ssim(video_path1, video_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1998470-4dfa-4a58-a1a5-60d39d5f3b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
